{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.4 \n",
						"Additional python modules to be included:\n",
						"transformers\n",
						"Current idle_timeout is None minutes.\n",
						"idle_timeout has been set to 2880 minutes.\n",
						"Setting Glue version to: 4.0\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.8X\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 32\n",
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Worker Type: G.8X\n",
						"Number of Workers: 32\n",
						"Session ID: e867bbab-5de8-44ee-befe-9e91fe1c5baf\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.4\n",
						"--enable-glue-datacatalog true\n",
						"--additional-python-modules transformers\n",
						"Waiting for session e867bbab-5de8-44ee-befe-9e91fe1c5baf to get into ready status...\n",
						"Session e867bbab-5de8-44ee-befe-9e91fe1c5baf has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"%additional_python_modules \"transformers\"\n",
				"\n",
				"%idle_timeout 2880\n",
				"%glue_version 4.0\n",
				"%worker_type G.8X\n",
				"%number_of_workers 32\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Libraries\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
					]
				}
			],
			"source": [
				"from pyspark.sql.functions import udf, explode, col\n",
				"from pyspark.sql.functions import sum as _sum\n",
				"from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
				"\n",
				"import re\n",
				"import transformers"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Load processed files from the s3 bucket\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 53,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"JSONL_NAME = \"papers-part10_filtered\"\n",
				"\n",
				"\n",
				"dynamic_frame = glueContext.create_dynamic_frame.from_options(\n",
				"        connection_type=\"s3\", \n",
				"        connection_options={\"paths\": [f\"s3://arcee-medical-dataset/raw-files/filtered_jsonl/{JSONL_NAME}.jsonl\"]},\n",
				"         format=\"json\"\n",
				"    )\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"dynamic_frame = glueContext.create_dynamic_frame.from_options(\n",
				"        connection_type=\"s3\",\n",
				"        connection_options={\n",
				"            \"paths\": [\"s3://arcee-medical-dataset/processed/books.csv\"],  # Path to your .txt files\n",
				"            \"recurse\": True  # Optional: To read files in subdirectories\n",
				"        },\n",
				"        format=\"csv\",  # Treat .txt files as single-column CSV\n",
				"        format_options={\n",
				"            \"withHeader\": True,  # Assuming .txt files do not have headers\n",
				"        }\n",
				"    )"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Example: Convert the DynamicFrame to a Spark DataFrame\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n"
					]
				}
			],
			"source": [
				"df = dynamic_frame.toDF()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Drop duplicates and null values from the dataframe in terms of the text column"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"spark_df =df.dropDuplicates([\"text\"]).dropna()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+--------+--------+--------------------+\n",
						"|corpusid|pubmedid|                text|\n",
						"+--------+--------+--------------------+\n",
						"|        |        |CHARLES CLARKE NE...|\n",
						"|        |        |GERMAN SHORTSTORI...|\n",
						"|        |        |Get Full Access a...|\n",
						"|        |        |-> wall pw siglan...|\n",
						"|        |        |BY JONATHAN BOARD...|\n",
						"+--------+--------+--------------------+\n",
						"only showing top 5 rows\n"
					]
				}
			],
			"source": [
				"from pyspark.sql.functions import expr\n",
				"\n",
				"# Replace newline characters with a space in each row of the 'text' column\n",
				"df_cleaned = spark_df.withColumn(\"text\", expr(\"replace(text, '\\n', ' ')\"))\n",
				"\n",
				"# Show the updated DataFrame to verify the changes\n",
				"df_cleaned.show(5)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+--------+--------+--------------------+--------------------+-----------+\n",
						"|corpusid|pubmedid|                text|              tokens|token_count|\n",
						"+--------+--------+--------------------+--------------------+-----------+\n",
						"|        |        |CHARLES CLARKE NE...|[1, 26871, 17101,...|     214043|\n",
						"|        |        |GERMAN SHORTSTORI...|[1, 402, 1001, 27...|      59371|\n",
						"|        |        |Get Full Access a...|[1, 3617, 14846, ...|     108223|\n",
						"|        |        |-> wall pw siglan...|[1, 1599, 10090, ...|      37748|\n",
						"|        |        |BY JONATHAN BOARD...|[1, 6770, 435, 11...|     212753|\n",
						"+--------+--------+--------------------+--------------------+-----------+\n",
						"only showing top 5 rows\n",
						"\n",
						"tokenizer_config.json: 100%|##########| 776/776 [00:00<00:00, 6.15MB/s]\n",
						"tokenizer.model: 100%|##########| 500k/500k [00:00<00:00, 195MB/s]\n",
						"tokenizer.json: 100%|##########| 1.84M/1.84M [00:00<00:00, 51.1MB/s]\n",
						"special_tokens_map.json: 100%|##########| 414/414 [00:00<00:00, 4.33MB/s]\n"
					]
				}
			],
			"source": [
				"from pyspark.sql.functions import udf, col\n",
				"from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
				"import transformers\n",
				"\n",
				"# Assuming the tokenizer initialization is successful\n",
				"tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=\"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\")\n",
				"\n",
				"# Define the schema of the UDF's return type\n",
				"schema = StructType([\n",
				"    StructField(\"tokens\", ArrayType(IntegerType()), nullable=True),\n",
				"    StructField(\"token_count\", IntegerType(), nullable=True)\n",
				"])\n",
				"\n",
				"# Modify the function to return both token IDs and token count\n",
				"def get_tokens_and_count(text):\n",
				"    if not isinstance(text, str):\n",
				"        return ([], 0)\n",
				"    try:\n",
				"        # Encode the text and return both the token IDs and the token count\n",
				"        encoded_input = tokenizer.encode(text, add_special_tokens=True)\n",
				"        return (encoded_input, len(encoded_input))\n",
				"    except Exception as e:\n",
				"        print(f\"Error encoding text: {e}\")\n",
				"        return ([], 0)\n",
				"\n",
				"# Define the UDF with the structured return type\n",
				"get_tokens_and_count_udf = udf(get_tokens_and_count, schema)\n",
				"\n",
				"# Apply the UDF to the DataFrame to create new 'tokens' and 'token_count' columns\n",
				"df_with_tokens_and_count = df_cleaned.withColumn(\"tokens_and_count\", get_tokens_and_count_udf(col(\"text\")))\n",
				"\n",
				"# Expand the struct into separate columns\n",
				"df_final = df_with_tokens_and_count.select(\n",
				"    \"*\", \n",
				"    \"tokens_and_count.tokens\", \n",
				"    \"tokens_and_count.token_count\"\n",
				").drop(\"tokens_and_count\")\n",
				"\n",
				"# Show the result\n",
				"df_final.show(5)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 41,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# from pyspark.sql.functions import sum\n",
				"\n",
				"# # Sum the token_count column to get the total number of tokens\n",
				"# total_tokens = df_final.agg(sum(\"token_count\")).collect()[0][0]\n",
				"\n",
				"# print(f\"Total number of tokens: {total_tokens}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 42,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# # Filter the DataFrame to only include rows with 0 tokens\n",
				"# df_final = df_final.filter(\"token_count = 0\")\n",
				"\n",
				"# # Show the filtered DataFrame\n",
				"# df_final.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Save"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"s3_bucket_path_all = f\"s3://arcee-medical-dataset/tokenized-parquet-files/{JSONL_NAME}.parquet\"\n",
				"\n",
				"df_final.write.mode('overwrite').parquet(s3_bucket_path_all)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
